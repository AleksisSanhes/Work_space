===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\bot\db.py =====
import json
import os

SENT_IDS_FILE = "data/sent_ids.json"
NEWS_DB_FILE = "data/news_db.json"

NEWS_DB = {}

def load_sent_ids() -> set:
    if os.path.exists(SENT_IDS_FILE):
        with open(SENT_IDS_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    return set()

def save_sent_ids(sent_ids: set):
    with open(SENT_IDS_FILE, "w", encoding="utf-8") as f:
        json.dump(list(sent_ids), f, ensure_ascii=False)

def load_news_db():
    global NEWS_DB
    if os.path.exists(NEWS_DB_FILE):
        with open(NEWS_DB_FILE, "r", encoding="utf-8") as f:
            NEWS_DB = json.load(f)

def save_news_db():
    with open(NEWS_DB_FILE, "w", encoding="utf-8") as f:
        json.dump(NEWS_DB, f, ensure_ascii=False)
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\bot\formatters.py =====
import re
import html

def safe_clean_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r"<[^>]+>", "", text)
    text = html.unescape(text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def format_news_for_publication(news_item: dict) -> str:
    title = safe_clean_text(news_item.get("title", "Без заголовка"))
    text = safe_clean_text(news_item.get("full_text", ""))
    source = safe_clean_text(news_item.get("source", "Источник не указан"))
    url = news_item.get("url", "")

    if len(text) > 3800:
        text = text[:3800] + "... [обрезано]"

    return f"🔥 {title}\n\n{text}\n\nИсточник: {source}\nОригинал: {url}"
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\bot\moderation_bot.py =====
import os
import json
import asyncio
import html
import re
import hashlib
import logging
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, Bot
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, ContextTypes

TOKEN = "8217915867:AAFLPnQmnxhHmjloF4Ct3HhR9jjRjVYV6C8"
MODERATION_CHANNEL = "-1002996332660"
PUBLISH_CHANNEL = "-1003006895565"

# Пути к файлам
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)
SENT_IDS_FILE = os.path.join(DATA_DIR, "sent_ids.json")
NEWS_DB_FILE = os.path.join(DATA_DIR, "news_db.json")

# --- Логирование ---
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO
)
logger = logging.getLogger(__name__)

# --- Глобальная база новостей ---
NEWS_DB = {}  # {id: {... данные ...}}

# ---------- Утилиты ----------
def make_news_id(item, index=0):
    key = (item.get("url") or "").strip()
    if not key:
        key = f"{item.get('title','')}-{item.get('date','')}".strip()
    if not key:
        key = item.get("preview", "")[:120]
    return hashlib.sha256(key.encode("utf-8")).hexdigest()[:16]


def safe_clean_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r"<[^>]+>", "", text)
    text = html.unescape(text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


# ---------- Работа с файлами ----------
def load_sent_ids() -> set:
    if os.path.exists(SENT_IDS_FILE):
        with open(SENT_IDS_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    return set()


def save_sent_ids(sent_ids: set):
    os.makedirs(os.path.dirname(SENT_IDS_FILE), exist_ok=True)
    with open(SENT_IDS_FILE, "w", encoding="utf-8") as f:
        json.dump(list(sent_ids), f, ensure_ascii=False)


def load_news_db():
    global NEWS_DB
    if os.path.exists(NEWS_DB_FILE):
        with open(NEWS_DB_FILE, "r", encoding="utf-8") as f:
            NEWS_DB = json.load(f)
        logger.info(f"Загружено NEWS_DB из файла ({len(NEWS_DB)} записей).")


def save_news_db():
    os.makedirs(os.path.dirname(NEWS_DB_FILE), exist_ok=True)
    with open(NEWS_DB_FILE, "w", encoding="utf-8") as f:
        json.dump(NEWS_DB, f, ensure_ascii=False)


# ---------- Отправка с задержкой ----------
async def send_with_delay(bot, chat_id, text, reply_markup=None, pause: float = 1.5):
    try:
        message = await bot.send_message(
            chat_id=chat_id,
            text=text,
            reply_markup=reply_markup,
            disable_web_page_preview=True,
        )
        await asyncio.sleep(pause)  # антирейтлимит
        return message
    except Exception as e:
        logger.error(f"Ошибка при отправке сообщения: {e}")
        return None


# ---------- Telegram Handlers ----------
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("Бот запущен.")


async def test_publish_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        await context.bot.send_message(
            chat_id=PUBLISH_CHANNEL, text="🔔 Тестовая публикация"
        )
        await update.message.reply_text("Отправлено (если бот имеет доступ к каналу).")
    except Exception as e:
        await update.message.reply_text(f"⚠️ Ошибка при публикации: {e}")


async def send_to_moderation(bot: Bot, news_item: dict, sent_ids: set):
    title = safe_clean_text(news_item.get("title", "Без заголовка"))
    source = safe_clean_text(news_item.get("source", "Неизвестный источник"))
    date = safe_clean_text(news_item.get("date", ""))
    preview = safe_clean_text(news_item.get("preview", ""))
    url = news_item.get("url", "")

    text = f"{title}\n\nИсточник: {source}\nДата: {date}\nСсылка: {url}\n\n{preview}"
    keyboard = [
        [
            InlineKeyboardButton(
                "✅ Опубликовать", callback_data=f"approve|{news_item['id']}"
            ),
            InlineKeyboardButton(
                "❌ Отклонить", callback_data=f"reject|{news_item['id']}"
            ),
        ]
    ]

    message = await send_with_delay(
        bot,
        MODERATION_CHANNEL,
        text,
        reply_markup=InlineKeyboardMarkup(keyboard),
    )

    if message:
        NEWS_DB[news_item["id"]] = {
            "message_id": message.message_id,
            "news_data": news_item,
            "channel_id": MODERATION_CHANNEL,
        }
        sent_ids.add(news_item["id"])
        logger.info(f"Новость {news_item['id']} отправлена в модерацию.")
        save_news_db()
        save_sent_ids(sent_ids)


def format_news_for_publication(news_item: dict) -> str:
    title = safe_clean_text(news_item.get("title", "Без заголовка"))
    text = safe_clean_text(news_item.get("full_text", ""))
    source = safe_clean_text(news_item.get("source", "Источник не указан"))
    url = news_item.get("url", "")

    if len(text) > 3800:
        text = text[:3800] + "... [обрезано]"

    return f"🔥 {title}\n\n{text}\n\nИсточник: {source}\nОригинал: {url}"


async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    try:
        action, news_id = query.data.split("|", 1)
        data_entry = NEWS_DB.get(news_id)
        if not data_entry:
            await query.edit_message_text("⚠️ Запись не найдена.")
            return

        channel_id = data_entry["channel_id"]
        message_id = data_entry["message_id"]
        news_item = data_entry["news_data"]

        if action == "approve":
            publication_text = format_news_for_publication(news_item)
            await context.bot.send_message(
                chat_id=PUBLISH_CHANNEL,
                text=publication_text,
                disable_web_page_preview=True,
            )
            await context.bot.edit_message_text(
                chat_id=channel_id,
                message_id=message_id,
                text=f"✅ ОПУБЛИКОВАНО\n\n{query.message.text}",
                reply_markup=None,
            )
            logger.info(f"Новость {news_id} опубликована.")

        elif action == "reject":
            await context.bot.edit_message_text(
                chat_id=channel_id,
                message_id=message_id,
                text=f"❌ ОТКЛОНЕНО\n\n{query.message.text}",
                reply_markup=None,
            )
            logger.info(f"Новость {news_id} отклонена.")

        save_news_db()

    except Exception as e:
        logger.error(f"Ошибка обработки callback: {e}")
        await query.edit_message_text(f"⚠️ Ошибка: {e}")


# ---------- Загрузка новостей из файлов ----------
async def load_and_send_news_if_requested(bot: Bot):
    answer = input(
        "\nВыберите режим:\n"
        "1) Загрузить новые новости\n"
        "2) Только слушать кнопки\n"
        "Введите 1 или 2: "
    ).strip()

    # --- Всегда загружаем базу и sent_ids ---
    load_news_db()
    sent_ids = load_sent_ids()

    files = [
        os.path.join(DATA_DIR, f)
        for f in os.listdir(DATA_DIR)
        if f.startswith("energy_news") and f.endswith(".json")
    ]

    if not files:
        print("❗ Нет файлов energy_news*.json")
        return

    latest_file = max(files, key=os.path.getctime)

    with open(latest_file, "r", encoding="utf-8") as f:
        news_list = json.load(f)

    if answer == "2":
        # --- Режим только слушать кнопки ---
        print("Режим только слушать кнопки. NEWS_DB заполнена для работы кнопок.")
        for i, item in enumerate(news_list):
            item_id = make_news_id(item, i)
            if item_id not in NEWS_DB:
                NEWS_DB[item_id] = {
                    "message_id": None,
                    "news_data": item,
                    "channel_id": MODERATION_CHANNEL,
                }
        save_news_db()
        return

    # --- Режим 1: отправка новых новостей ---
    count = 0
    for i, item in enumerate(news_list):
        if not all(
            k in item for k in ["title", "source", "date", "url", "preview", "full_text"]
        ):
            continue
        item_id = make_news_id(item, i)
        if item_id in sent_ids:
            continue
        item["id"] = item_id
        await send_to_moderation(bot, item, sent_ids)
        count += 1

    print(f"Отправлено в модерацию: {count} новых новостей.")


# ---------- Post init ----------
async def post_init(app: Application):
    try:
        await app.bot.delete_webhook(drop_pending_updates=True)
    except Exception as e:
        logger.warning(f"Не удалось удалить webhook: {e}")

    await load_and_send_news_if_requested(app.bot)


# ---------- Функция запуска бота ----------
def run_bot():
    application = (
        Application.builder().token(TOKEN).post_init(post_init).build()
    )

    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("testpublish", test_publish_command))
    application.add_handler(
        CallbackQueryHandler(button_handler, pattern=r"^(approve|reject)\|")
    )

    application.run_polling()
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\bot\__init__.py =====
# Пакет для Telegram-бота модерации и публикации
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\parser\html_parser_custom.py =====
# parser/html_parser_custom.py
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from parser.utils import clean_text
from parser.nlp_filter import is_energy_related

def parse_eenergy_media():
    url = "https://eenergy.media/rubric/news"
    news_list = []

    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(r.text, "html.parser")
        articles = soup.select("div.t-news__item")  # CSS селектор для блоков новостей

        for a in articles:
            title_tag = a.select_one("a.t-news__title")
            if not title_tag:
                continue
            title = clean_text(title_tag.get_text())
            link = title_tag.get("href")
            preview = clean_text(a.select_one("div.t-news__preview").get_text()) if a.select_one("div.t-news__preview") else ""
            date_str = a.select_one("time")["datetime"] if a.select_one("time") else ""
            date = date_str[:16] if date_str else datetime.now().strftime("%Y-%m-%d %H:%M")

            relevant, reason = is_energy_related(title + " " + preview)
            if relevant:
                news_list.append({
                    "title": title,
                    "url": link,
                    "date": date,
                    "source": "E-Energy",
                    "preview": preview[:300] + "...",
                    "full_text": "",
                    "relevance_reason": reason,
                })
    except Exception as e:
        print(f"Ошибка парсинга E-Energy: {e}")

    return news_list


def parse_in_power():
    url = "https://www.in-power.ru/news/alternativnayaenergetika"
    news_list = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(r.text, "html.parser")
        articles = soup.select("div.news-list-item")

        for a in articles:
            title_tag = a.select_one("a.news-title")
            if not title_tag:
                continue
            title = clean_text(title_tag.get_text())
            link = title_tag.get("href")
            preview = clean_text(a.select_one("div.news-text").get_text()) if a.select_one("div.news-text") else ""
            date_tag = a.select_one("div.news-date")
            date = clean_text(date_tag.get_text()) if date_tag else datetime.now().strftime("%Y-%m-%d %H:%M")

            relevant, reason = is_energy_related(title + " " + preview)
            if relevant:
                news_list.append({
                    "title": title,
                    "url": link,
                    "date": date,
                    "source": "In-Power",
                    "preview": preview[:300] + "...",
                    "full_text": "",
                    "relevance_reason": reason,
                })
    except Exception as e:
        print(f"Ошибка парсинга In-Power: {e}")

    return news_list


def parse_neftegaz():
    url = "https://neftegaz.ru/news/Alternative-energy/"
    news_list = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(r.text, "html.parser")
        articles = soup.select("div.article-preview")

        for a in articles:
            title_tag = a.select_one("a.article-title")
            if not title_tag:
                continue
            title = clean_text(title_tag.get_text())
            link = title_tag.get("href")
            preview = clean_text(a.select_one("div.article-text").get_text()) if a.select_one("div.article-text") else ""
            date_tag = a.select_one("div.article-date")
            date = clean_text(date_tag.get_text()) if date_tag else datetime.now().strftime("%Y-%m-%d %H:%M")

            relevant, reason = is_energy_related(title + " " + preview)
            if relevant:
                news_list.append({
                    "title": title,
                    "url": link,
                    "date": date,
                    "source": "Neftegaz",
                    "preview": preview[:300] + "...",
                    "full_text": "",
                    "relevance_reason": reason,
                })
    except Exception as e:
        print(f"Ошибка парсинга Neftegaz: {e}")

    return news_list


def parse_oilcapital():
    url = "https://oilcapital.ru/tags/vie"
    news_list = []
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(r.text, "html.parser")
        articles = soup.select("div.news-item")

        for a in articles:
            title_tag = a.select_one("a.title")
            if not title_tag:
                continue
            title = clean_text(title_tag.get_text())
            link = title_tag.get("href")
            preview = clean_text(a.select_one("div.preview").get_text()) if a.select_one("div.preview") else ""
            date_tag = a.select_one("span.date")
            date = clean_text(date_tag.get_text()) if date_tag else datetime.now().strftime("%Y-%m-%d %H:%M")

            relevant, reason = is_energy_related(title + " " + preview)
            if relevant:
                news_list.append({
                    "title": title,
                    "url": link,
                    "date": date,
                    "source": "Oilcapital",
                    "preview": preview[:300] + "...",
                    "full_text": "",
                    "relevance_reason": reason,
                })
    except Exception as e:
        print(f"Ошибка парсинга Oilcapital: {e}")

    return news_list


def parse_all_custom_sites():
    all_news = []
    all_news.extend(parse_eenergy_media())
    all_news.extend(parse_in_power())
    all_news.extend(parse_neftegaz())
    all_news.extend(parse_oilcapital())
    return all_news
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\parser\logger_monitor.py =====
# parser/logger_monitor.py
import logging
import sys
import time
from functools import wraps
from tqdm import tqdm
import parser.rss_parser as rss
import parser.nlp_filter as nlp
import os

# --- Настройка логирования ---
LOG_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
LOG_FILE = os.path.join(LOG_DIR, "parser_monitor.log")


def setup_logger():
    os.makedirs(LOG_DIR, exist_ok=True)   # ✅ гарантируем, что data/ есть

    logger = logging.getLogger("parser_monitor")
    logger.setLevel(logging.DEBUG)

    # Файл
    fh = logging.FileHandler(LOG_FILE, encoding="utf-8")
    fh.setLevel(logging.DEBUG)

    # Консоль
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)

    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)

    logger.addHandler(fh)
    logger.addHandler(ch)

    return logger


logger = setup_logger()

# --- Декоратор для замера времени функций ---
def log_time(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        logger.info(f"START: {func.__name__}")
        result = func(*args, **kwargs)
        end = time.time()
        logger.info(f"END: {func.__name__} | duration: {end-start:.2f} sec")
        return result
    return wrapper

# --- Обёртка для get_full_text ---
def monitored_get_full_text(func):
    @wraps(func)
    def wrapper(url, *args, **kwargs):
        logger.info(f"Fetching full text: {url}")
        start = time.time()
        text = func(url, *args, **kwargs)
        duration = time.time() - start
        logger.info(f"Fetched {len(text)} chars | duration: {duration:.2f} sec")
        return text
    return wrapper

# --- Обёртка для классификации ---
def monitored_is_energy_related(func):
    @wraps(func)
    def wrapper(text, classifier=None, *args, **kwargs):
        start = time.time()
        relevant, reason = func(text, classifier, *args, **kwargs)
        duration = time.time() - start
        logger.info(f"Classification: relevant={relevant} | reason={reason} | duration={duration:.2f} sec")
        return relevant, reason
    return wrapper

# logger_monitor.py (только исправленная обёртка для parse_all_feeds)
def monitored_parse_all_feeds(func):
    @wraps(func)
    def wrapper(classifier=None, stats=None, *args, **kwargs):
        from tqdm import tqdm
        # Оборачиваем RSS_FEEDS в tqdm
        rss.RSS_FEEDS = list(tqdm(rss.RSS_FEEDS, desc="RSS Feeds"))
        logger.info("Starting parse_all_feeds")
        all_news = func(classifier=classifier, stats=stats, *args, **kwargs)
        logger.info(f"Finished parse_all_feeds | total news: {len(all_news)}")
        return all_news
    return wrapper


# --- Применяем обёртки ---
rss.get_full_text = monitored_get_full_text(rss.get_full_text)
nlp.is_energy_related = monitored_is_energy_related(nlp.is_energy_related)
rss.parse_all_feeds = monitored_parse_all_feeds(rss.parse_all_feeds)
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\parser\nlp_filter.py =====
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from deep_translator import GoogleTranslator
import logging

logger = logging.getLogger(__name__)

EXPANDED_KEYWORDS = [
    "энергетик", "виэ", "водород", "акб", "экологи", "декарбонизац", "возобнов", "электромобил",
    "экотех", "климат", "энергопереход", "renewable", "solar", "wind", "battery", "hydrogen",
    "decarbonization", "sustainability", "green energy", "clean tech", "photovoltaic", "wind turbine",
    "энергоэффективность", "биотопливо", "геотермальный", "приливная энергия", "энергосбережение"
]

def load_classification_model():
    logger.info("Загрузка модели классификации...")
    try:
        model_name = "cointegrated/rubert-tiny2-cedr-emotion-detection"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        classifier = pipeline(
            "text-classification",
            model=model,
            tokenizer=tokenizer,
            device=0 if torch.cuda.is_available() else -1
        )
        logger.info("Модель загружена")
        return classifier
    except Exception as e:
        logger.error(f"Ошибка загрузки модели: {str(e)}")
        return None


def is_energy_related(text, classifier=None, threshold=0.90):
    if not text.strip():
        return False, "Пустой текст"
    text_lower = text.lower()
    keyword_count = sum(1 for keyword in EXPANDED_KEYWORDS if keyword.lower() in text_lower)
    if keyword_count >= 2:
        return True, f"Найдено {keyword_count} ключевых слов"
    if not classifier:
        return keyword_count > 0, "Проверка только по ключевым словам"
    try:
        result = classifier(text[:400], truncation=True, max_length=512)
        for res in result:
            if res['label'] == 'neutral' and res['score'] > threshold:
                return True, f"ИИ-классификация ({res['score']:.2f})"
        return False, "Нерелевантно"
    except Exception as e:
        logger.error(f"Ошибка классификации: {str(e)}")
        return keyword_count > 0, "Ошибка ИИ"

def translate_text(text, src="en", dest="ru", source_name=None, article_url=None):
    if not text.strip():
        return text
    try:
        max_chunk_size = 4500
        chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]
        translated = [GoogleTranslator(source=src, target=dest).translate(chunk) for chunk in chunks]
        return " ".join(translated)
    except Exception as e:
        logger.warning(f"Ошибка перевода ({source_name or '???'} - {article_url or ''}): {str(e)}")
        return text
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\parser\rss_parser.py =====
import feedparser
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup

from .utils import clean_text
from .nlp_filter import is_energy_related, translate_text
from .stats import update_stats
#
RSS_FEEDS = [
    {"url": "https://lenta.ru/rss/news", "name": "Lenta.ru"},
    {"url": "https://www.interfax.ru/rss.asp", "name": "Interfax"},
    {"url": "https://ria.ru/export/rss2/archive/index.xml", "name": "RIA Novosti"},
    {"url": "https://www.vedomosti.ru/rss/news", "name": "Vedomosti"},
    {"url": "https://hightech.fm/feed", "name": "Hi-Tech Mail.ru"},
    {"url": "https://recyclemag.ru/rss.xml", "name": "Recyclemag"},
    {"url": "https://www.kommersant.ru/RSS/news/energy", "name": "Коммерсантъ Энергетика"},
    {"url": "https://tass.ru/rss/economy.xml", "name": "ТАСС Экономика"},
    {"url": "https://ria.ru/export/rss2/tech/index.xml", "name": "РИА Наука"},
    {"url": "http://energosovet.ru/rss.xml", "name": "Энергосовет"},
    {"url": "https://renen.ru/feed/", "name": "RENEN - ВИЭ"},
    {"url": "https://greenevolution.ru/feed/", "name": "Green Evolution"},
    {"url": "https://energovector.com/feed/", "name": "Энерговектор"},
    {"url": "https://www.reutersagency.com/feed/?best-topics=energy&post_type=best", "name": "Reuters Energy"},
    {"url": "https://www.bloomberg.com/feeds/bbiz/sustainability.xml", "name": "Bloomberg Green"},
    {"url": "https://cleantechnica.com/feed/", "name": "CleanTechnica"},
    {"url": "https://www.h2-view.com/feed/", "name": "H2 View"},
    {"url": "https://energynews.us/feed/", "name": "Energy News Network"},
    {"url": "https://www.greentechmedia.com/feed", "name": "Greentech Media"},
    {"url": "https://www.hydrogenfuelnews.com/feed/", "name": "Hydrogen Fuel News"},
    {"url": "https://www.pv-magazine.com/feed/", "name": "PV Magazine"},
    {"url": "https://www.renewableenergyworld.com/feed/", "name": "Renewable Energy World"},
    {"url": "https://www.greencarcongress.com/index.xml", "name": "Green Car Congress"},
    {"url": "https://www.energy-storage.news/feed/", "name": "Energy Storage News"},
    {"url": "https://eenergy.media/rubric/news", "name": "E-Energy"},
    {"url": "https://www.in-power.ru/news/alternativnayaenergetika", "name": "In-Power"},
    {"url": "https://neftegaz.ru/news/Alternative-energy/", "name": "Neftegaz"},
    {"url": "https://oilcapital.ru/rss", "name": "Oilcapital"},
]

def get_full_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=15)

        # Попробовать использовать кодировку из meta/headers
        response.encoding = response.apparent_encoding

        soup = BeautifulSoup(response.text, "html.parser")
        text = " ".join([p.get_text(strip=True) for p in soup.select("p") if len(p.get_text(strip=True)) > 30])
        return clean_text(text)
    except Exception as e:
        print(f"Ошибка get_full_text({url}): {e}")
        return ""


def parse_feed(feed_url, source_name, classifier, stats):
    results = []
    headers = {"User-Agent": "Mozilla/5.0"}
    three_weeks_ago = datetime.now() - timedelta(days=21)

    response = requests.get(feed_url, headers=headers, timeout=20)

    # Передаём байты — feedparser сам правильно определит кодировку
    feed = feedparser.parse(response.content)

    for entry in feed.entries:
        pub_date = datetime.now()
        if hasattr(entry, "published_parsed"):
            pub_date = datetime(*entry.published_parsed[:6])

        if pub_date < three_weeks_ago:
            update_stats(stats, source_name, "old_date")
            continue

        content = entry.title + " " + getattr(entry, "summary", "")
        full_text = get_full_text(entry.link)
        combined_text = content + " " + full_text

        relevant, reason = is_energy_related(combined_text, classifier)
        if relevant:
            news = {
                "title": entry.title,
                "url": entry.link,
                "date": pub_date.strftime("%Y-%m-%d %H:%M"),
                "source": source_name,
                "preview": combined_text[:300] + "...",
                "full_text": full_text,
                "relevance_reason": reason,
            }
            results.append(news)
            update_stats(stats, source_name, "accepted")
        else:
            update_stats(stats, source_name, "not_relevant")

    return results

def parse_all_feeds(classifier=None, stats=None):
    all_news = []
    for feed in RSS_FEEDS:
        news = parse_feed(feed["url"], feed["name"], classifier, stats)
        all_news.extend(news)
    return all_news
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\parser\stats.py =====
import json
from collections import defaultdict
from datetime import datetime
import os

def init_stats():
    return {
        "total_articles": 0,
        "accepted": 0,
        "rejected": defaultdict(int),
        "failed_sources": [],
        "source_details": {},
        "start_time": datetime.now()
    }

def update_stats(stats, source, reason):
    source_stats = stats["source_details"].setdefault(
        source, {"total": 0, "accepted": 0, "rejected": defaultdict(int), "errors": []}
    )
    stats["total_articles"] += 1
    source_stats["total"] += 1

    if reason == "accepted":
        stats["accepted"] += 1
        source_stats["accepted"] += 1
    else:
        stats["rejected"][reason] += 1
        source_stats["rejected"][reason] += 1

def generate_stats_report(stats):
    report = "\n===== СТАТИСТИКА ОБРАБОТКИ =====\n"
    report += f"Всего статей: {stats['total_articles']}\n"
    report += f"Принято: {stats['accepted']}\n"
    report += f"Отклонено: {stats['total_articles'] - stats['accepted']}\n"
    return report

def save_results(all_news, stats, timestamp):
    # Создаём папку data, если её нет
    os.makedirs("data", exist_ok=True)

    json_filename = f"data/energy_news_{timestamp}.json"
    stats_filename = f"data/processing_stats_{timestamp}.txt"

    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(all_news, f, indent=2, ensure_ascii=False)

    with open(stats_filename, "w", encoding="utf-8") as f:
        f.write(generate_stats_report(stats))

    return json_filename, stats_filename
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\parser\utils.py =====
import re
import html

def clean_text(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"https?://\S+", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return html.unescape(text)
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\parser\__init__.py =====
# Пакет для парсинга и анализа новостей
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\main_bot.py =====
from bot.moderation_bot import run_bot

if __name__ == "__main__":
    run_bot()
===== C:\Users\SK-GROUP\PycharmProjects\PythonProject\energy_news_project\main_parser.py =====
# main_parser.py
from parser.rss_parser import parse_all_feeds
from parser.html_parser_custom import parse_all_custom_sites
from parser.stats import init_stats, generate_stats_report, save_results
from parser.nlp_filter import load_classification_model
from parser.logger_monitor import logger
from datetime import datetime

if __name__ == "__main__":
    logger.info("Запуск парсера новостей")

    # --- Инициализация статистики и модели ---
    stats = init_stats()
    classifier = load_classification_model()

    # --- 1) Парсим RSS-фиды ---
    logger.info("Парсинг RSS-фидов")
    rss_news = parse_all_feeds(classifier=classifier, stats=stats)
    logger.info(f"Найдено {len(rss_news)} новостей из RSS")

    # --- 2) Парсим кастомные HTML-сайты ---
    logger.info("Парсинг HTML-сайтов")
    html_news = parse_all_custom_sites()
    logger.info(f"Найдено {len(html_news)} новостей с HTML-сайтов")

    # --- 3) Объединяем все новости ---
    all_news = rss_news + html_news
    logger.info(f"Всего новостей: {len(all_news)}")

    # --- 4) Сохраняем результаты ---
    if all_news:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        json_file, stats_file = save_results(all_news, stats, timestamp)
        logger.info(f"Сохранено {len(all_news)} новостей в {json_file}")
        logger.info(generate_stats_report(stats))
    else:
        logger.info("Новости не найдены")
