import feedparser
from datetime import datetime, timedelta
import requests
import json
from bs4 import BeautifulSoup

# –†–∞–±–æ—á–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–∏
RSS_FEEDS = [
    {"url": "https://lenta.ru/rss/news", "name": "Lenta.ru"},
    {"url": "https://www.interfax.ru/rss.asp", "name": "Interfax"},
    {"url": "https://ria.ru/export/rss2/archive/index.xml", "name": "RIA Novosti"},
    {"url": "https://www.vedomosti.ru/rss/news", "name": "Vedomosti"},
    {"url": "https://hightech.fm/feed", "name": "Hi-Tech Mail.ru"},  # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏
    {"url": "https://recyclemag.ru/rss.xml", "name": "Recyclemag"}  # –≠–∫–æ–ª–æ–≥–∏—è –∏ –í–ò–≠
]

KEYWORDS = ["—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫", "–≤–∏—ç", "–≤–æ–¥–æ—Ä–æ–¥", "–∞–∫–±", "—ç–∫–æ–ª–æ–≥–∏", "–¥–µ–∫–∞—Ä–±–æ–Ω–∏–∑–∞—Ü",
            "–≤–æ–∑–æ–±–Ω–æ–≤", "—ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª", "—ç–∫–æ—Ç–µ—Ö", "–∫–ª–∏–º–∞—Ç", "—ç–Ω–µ—Ä–≥–æ–ø–µ—Ä–µ—Ö–æ–¥",
            "renewable", "solar", "wind", "battery", "hydrogen", "decarbonization"]


def parse_rss_feed(feed_url, source_name):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(feed_url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        feed = feedparser.parse(response.text)

        results = []
        three_weeks_ago = datetime.now() - timedelta(days=21)

        for entry in feed.entries:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç—ã
            if hasattr(entry, 'published_parsed'):
                pub_date = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed'):
                pub_date = datetime(*entry.updated_parsed[:6])
            else:
                pub_date = datetime.now()

            if pub_date < three_weeks_ago:
                continue

            # –°–±–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            content = ""
            if hasattr(entry, 'title'):
                content += entry.title.lower() + " "
            if hasattr(entry, 'summary'):
                content += entry.summary.lower() + " "
            if hasattr(entry, 'description'):
                content += entry.description.lower() + " "

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if any(keyword in content for keyword in KEYWORDS):
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                full_text = ""
                if 'ria.ru' in feed_url or 'hightech.fm' in feed_url:
                    try:
                        article_resp = requests.get(entry.link, headers=headers, timeout=5)
                        soup = BeautifulSoup(article_resp.text, 'html.parser')

                        if 'ria.ru' in feed_url:
                            article_div = soup.find('div', class_='article__text')
                            if article_div:
                                full_text = article_div.get_text(separator=' ', strip=True)[:500] + "..."
                        elif 'hightech.fm' in feed_url:
                            article_div = soup.find('div', class_='post__text')
                            if article_div:
                                full_text = article_div.get_text(separator=' ', strip=True)[:500] + "..."
                    except:
                        pass

                results.append({
                    "title": entry.title if hasattr(entry, 'title') else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è",
                    "url": entry.link,
                    "date": pub_date.strftime("%Y-%m-%d %H:%M"),
                    "source": source_name,
                    "preview": content[:200] + "..." if len(content) > 200 else content,
                    "full_text": full_text
                })

        return results

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {source_name}: {str(e)}")
        return []


# –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
all_news = []
for feed in RSS_FEEDS:
    print(f"üîé –ü—Ä–æ–≤–µ—Ä–∫–∞: {feed['name']}")
    news = parse_rss_feed(feed['url'], feed['name'])
    all_news.extend(news)
    print(f"   –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news)}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
with open('energy_news.json', 'w', encoding='utf-8') as f:
    json.dump(all_news, f, indent=2, ensure_ascii=False, default=str)

print(f"\nüíæ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_news)}")
print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ energy_news.json")